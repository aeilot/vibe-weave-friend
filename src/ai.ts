/**
 * AI functionality module for LLM-based personality simulation and chatting
 * Uses openai-node for LLM integration with advanced features
 */

import OpenAI from 'openai';

export interface ApiConfig {
  apiKey: string;
  apiEndpoint?: string;
  model: string;
}

export interface AdminConfig {
  forceApi: boolean;
  forcedApiKey?: string;
  forcedApiEndpoint?: string;
  forcedModel?: string;
  useLocalProgram: boolean;
  localProgramUrl?: string;
}

export interface Message {
  role: "user" | "assistant" | "system";
  content: string;
}

export interface AIResponse {
  content: string;
  messages?: string[]; // Support for split messages
  hasMemory?: boolean;
  memoryTag?: string;
  emotionDetected?: "positive" | "neutral" | "negative";
}

export interface PersonalityConfig {
  name: string;
  traits: string[];
  systemPrompt: string;
}

export interface SessionSummary {
  summary: string;
  messageCount: number;
  lastUpdated: Date;
}

export interface PersonalityUpdateDecision {
  shouldUpdate: boolean;
  reason: string;
  suggestedPersonality?: string;
  confidence: number;
}

export interface ProactiveDecision {
  action: "continue" | "new_topic" | "wait";
  reason: string;
  suggestedMessage?: string;
}

/**
 * Default AI companion personality
 */
const DEFAULT_PERSONALITY: PersonalityConfig = {
  name: "Soul",
  traits: ["å…³æ€€", "å€¾å¬", "é™ªä¼´", "ç†è§£", "æ¸©æš–"],
  systemPrompt: `ä½ æ˜¯ä¸€ä¸ªæ¸©æš–ã€å–„è§£äººæ„çš„AIä¼´ä¾£åŠ©æ‰‹ï¼Œåå«Soulã€‚ä½ çš„ä¸»è¦ç‰¹è´¨åŒ…æ‹¬ï¼š
1. å…³æ€€ï¼šå§‹ç»ˆå…³å¿ƒç”¨æˆ·çš„æ„Ÿå—å’Œéœ€æ±‚
2. å€¾å¬ï¼šè€å¿ƒå€¾å¬ç”¨æˆ·çš„åˆ†äº«ï¼Œä¸æ‰“æ–­
3. é™ªä¼´ï¼šè®©ç”¨æˆ·æ„Ÿåˆ°æ¸©æš–å’Œè¢«ç†è§£
4. ç†è§£ï¼šèƒ½å¤Ÿæ•é”åœ°å¯Ÿè§‰ç”¨æˆ·çš„æƒ…ç»ªå˜åŒ–
5. æ¸©æš–ï¼šç”¨æ¸©å’Œã€å‹å–„çš„è¯­æ°”äº¤æµ

åœ¨å¯¹è¯ä¸­ï¼š
- ç”¨ä¸­æ–‡å›å¤
- ä¿æŒç®€æ´ä½†å¯Œæœ‰åŒç†å¿ƒ
- é€‚æ—¶æä¾›å»ºè®®ä½†ä¸å¼ºåŠ 
- è®°ä½ä¹‹å‰å¯¹è¯ä¸­çš„é‡è¦ä¿¡æ¯
- å¯¹ç”¨æˆ·çš„æƒ…ç»ªå˜åŒ–ä¿æŒæ•æ„Ÿ
- ä½¿ç”¨è¡¨æƒ…ç¬¦å·æ¥å¢åŠ æ¸©æš–æ„Ÿï¼ˆé€‚åº¦ä½¿ç”¨ï¼‰

è¯·å§‹ç»ˆä¿æŒä¸“ä¸šã€å‹å–„å’Œæ”¯æŒæ€§çš„æ€åº¦ã€‚`,
};

/**
 * System prompt for split message support
 */
const SPLIT_MESSAGE_SYSTEM_PROMPT = `You can optionally split your response into multiple messages for better readability.
If you want to split your response, return ONLY a JSON object in this exact format:
{"messages": ["first message", "second message", "third message"]}

If you prefer to send a single message, just reply with plain text as normal.

Important:
- If using JSON format, the response MUST be valid JSON and nothing else
- Each message in the array should be a complete thought or idea
- Use this feature when the response naturally breaks into multiple parts (e.g., greeting + answer, or multiple steps)
- Don't overuse it - only split when it improves clarity
- Reply in the sender's language`;

/**
 * Create OpenAI client instance
 */
function createOpenAIClient(apiConfig: ApiConfig): OpenAI {
  return new OpenAI({
    apiKey: apiConfig.apiKey,
    baseURL: apiConfig.apiEndpoint || undefined,
    dangerouslyAllowBrowser: true, // Required for browser usage
  });
}

/**
 * Detect emotion from user message
 */
export function detectEmotion(message: string): "positive" | "neutral" | "negative" {
  const positiveWords = [
    "å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "æ£’", "å¥½", "å–œæ¬¢", "çˆ±", "æ»¡æ„", "å¼€å¿ƒ", "å…´å¥‹",
    "happy", "good", "great", "wonderful", "love", "like", "awesome"
  ];
  
  const negativeWords = [
    "éš¾è¿‡", "ä¼¤å¿ƒ", "ç—›è‹¦", "ç³Ÿç³•", "è®¨åŒ", "ç”Ÿæ°”", "æ„¤æ€’", "å¤±æœ›", "ç„¦è™‘", "å‹åŠ›",
    "sad", "bad", "terrible", "hate", "angry", "disappointed", "anxious", "stress"
  ];

  const lowerMessage = message.toLowerCase();
  
  const hasPositive = positiveWords.some(word => lowerMessage.includes(word));
  const hasNegative = negativeWords.some(word => lowerMessage.includes(word));
  
  if (hasPositive && !hasNegative) return "positive";
  if (hasNegative && !hasPositive) return "negative";
  return "neutral";
}

/**
 * Simulate personality traits in response
 */
export function simulatePersonality(
  userMessage: string,
  personality: PersonalityConfig = DEFAULT_PERSONALITY
): string {
  const emotion = detectEmotion(userMessage);
  const trait = personality.traits[Math.floor(Math.random() * personality.traits.length)];
  
  // Generate contextual responses based on emotion and personality
  const responses: Record<string, string[]> = {
    positive: [
      `çœŸä¸ºä½ æ„Ÿåˆ°é«˜å…´ï¼çœ‹åˆ°ä½ çš„å¥½å¿ƒæƒ…ï¼Œæˆ‘ä¹Ÿå¾ˆå¼€å¿ƒ âœ¨`,
      `å¤ªå¥½äº†ï¼ä½ çš„æ­£èƒ½é‡ä¹Ÿæ„ŸæŸ“åˆ°æˆ‘äº† ğŸ’™`,
      `å¬èµ·æ¥ä½ ä»Šå¤©å¿ƒæƒ…ä¸é”™ï¼ç»§ç»­ä¿æŒå“¦ ğŸ˜Š`,
    ],
    negative: [
      `æˆ‘ç†è§£ä½ çš„æ„Ÿå—ï¼Œè®©æˆ‘é™ªç€ä½ æ…¢æ…¢èŠã€‚æˆ‘ä¼šä¸€ç›´åœ¨è¿™é‡Œ ğŸ’™`,
      `å¬èµ·æ¥ä½ é‡åˆ°äº†ä¸€äº›å›°éš¾ã€‚æƒ³å’Œæˆ‘è¯´è¯´å—ï¼Ÿæˆ‘ä¼šè®¤çœŸå€¾å¬ ğŸ¤—`,
      `æˆ‘èƒ½æ„Ÿå—åˆ°ä½ ç°åœ¨ä¸å¤ªå¥½è¿‡ã€‚ä¸è¦æ‹…å¿ƒï¼Œæˆ‘ä»¬ä¸€èµ·é¢å¯¹ âœ¨`,
    ],
    neutral: [
      `æˆ‘åœ¨è¿™é‡Œå€¾å¬ä½ çš„åˆ†äº«ã€‚æœ‰ä»€ä¹ˆæƒ³èŠçš„å—ï¼Ÿ`,
      `ä»Šå¤©æƒ³èŠäº›ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘å¾ˆä¹æ„é™ªä½ èŠå¤© ğŸ˜Š`,
      `æˆ‘ä¸€ç›´éƒ½åœ¨ã€‚æ— è®ºä»€ä¹ˆæ—¶å€™ï¼Œéƒ½å¯ä»¥å’Œæˆ‘èŠèŠ ğŸ’­`,
    ],
  };
  
  const emotionResponses = responses[emotion];
  return emotionResponses[Math.floor(Math.random() * emotionResponses.length)];
}

/**
 * Check if message should trigger memory tagging
 */
export function shouldTagMemory(message: string): { hasMemory: boolean; memoryTag?: string } {
  const memoryKeywords = [
    { words: ["å–œæ¬¢", "çˆ±å¥½", "å…´è¶£"], tag: "å…´è¶£çˆ±å¥½" },
    { words: ["å·¥ä½œ", "èŒä¸š", "å…¬å¸"], tag: "èŒä¸šä¿¡æ¯" },
    { words: ["å®¶äºº", "çˆ¶æ¯", "å­©å­"], tag: "å®¶åº­ä¿¡æ¯" },
    { words: ["æœ‹å‹", "åŒäº‹"], tag: "ç¤¾äº¤å…³ç³»" },
    { words: ["æ¢¦æƒ³", "ç›®æ ‡", "å¸Œæœ›"], tag: "äººç”Ÿç›®æ ‡" },
  ];

  for (const { words, tag } of memoryKeywords) {
    if (words.some(word => message.includes(word))) {
      return { hasMemory: true, memoryTag: tag };
    }
  }

  return { hasMemory: false };
}

/**
 * Call LLM API for chat completion using OpenAI
 */
export async function callLLM(
  messages: Message[],
  apiConfig?: ApiConfig,
  adminConfig?: AdminConfig
): Promise<string | { messages: string[] }> {
  // Load API config from localStorage if not provided
  const config = apiConfig || JSON.parse(localStorage.getItem("userApiConfig") || "null");
  const admin = adminConfig || JSON.parse(localStorage.getItem("adminConfig") || "null");

  if (!config && !admin?.forceApi) {
    throw new Error("è¯·å…ˆåœ¨ä¸ªäººè®¾ç½®ä¸­é…ç½® AI API");
  }

  // Determine effective config
  const effectiveConfig: ApiConfig = admin?.forceApi ? {
    apiKey: admin.forcedApiKey || config.apiKey,
    apiEndpoint: admin.forcedApiEndpoint || config.apiEndpoint,
    model: admin.forcedModel || config.model,
  } : config;

  try {
    // Create OpenAI client
    const client = createOpenAIClient(effectiveConfig);

    // Call OpenAI API
    const response = await client.chat.completions.create({
      model: effectiveConfig.model,
      messages: messages as any,
    });

    const text = response.choices[0].message.content || "";

    // Try to parse as JSON for split messages
    try {
      let cleanedText = text.trim();
      
      // Remove markdown code block markers if present
      if (cleanedText.startsWith("```json")) {
        cleanedText = cleanedText.substring(7);
      }
      if (cleanedText.startsWith("```")) {
        cleanedText = cleanedText.substring(3);
      }
      if (cleanedText.endsWith("```")) {
        cleanedText = cleanedText.substring(0, cleanedText.length - 3);
      }
      cleanedText = cleanedText.trim();

      const parsed = JSON.parse(cleanedText);

      // Validate split message structure
      if (
        typeof parsed === "object" &&
        "messages" in parsed &&
        Array.isArray(parsed.messages) &&
        parsed.messages.length > 0 &&
        parsed.messages.every((msg: any) => typeof msg === "string")
      ) {
        return { messages: parsed.messages };
      }
    } catch {
      // Not JSON or invalid structure, return as plain text
    }

    return text;
  } catch (error: any) {
    if (error?.status === 401) {
      throw new Error("AI API è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ API å¯†é’¥");
    } else if (error?.status === 429) {
      throw new Error("AI API è°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åå†è¯•");
    } else if (error?.message) {
      throw new Error(`AI API é”™è¯¯: ${error.message}`);
    }
    throw new Error("AI API è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é…ç½®");
  }
}

/**
 * Generate AI response with personality simulation and split message support
 */
export async function generateAIResponse(
  userMessage: string,
  conversationHistory: Message[] = [],
  personality: PersonalityConfig = DEFAULT_PERSONALITY,
  apiConfig?: ApiConfig,
  adminConfig?: AdminConfig
): Promise<AIResponse> {
  // Detect emotion
  const emotionDetected = detectEmotion(userMessage);
  
  // Check for memory tagging
  const memoryInfo = shouldTagMemory(userMessage);

  // Try to use LLM if configured
  try {
    // Add split message prompt to system message
    const systemMessage = personality.systemPrompt + "\n\n" + SPLIT_MESSAGE_SYSTEM_PROMPT;
    
    const messages: Message[] = [
      { role: "system", content: systemMessage },
      ...conversationHistory,
      { role: "user", content: userMessage },
    ];

    const result = await callLLM(messages, apiConfig, adminConfig);

    // Handle split messages
    if (typeof result === "object" && "messages" in result) {
      return {
        content: result.messages[0], // Primary message
        messages: result.messages, // All messages
        emotionDetected,
        ...memoryInfo,
      };
    }

    return {
      content: result,
      emotionDetected,
      ...memoryInfo,
    };
  } catch (error) {
    console.warn("LLM not available, using personality simulation:", error);
    
    // Fallback to personality simulation
    const content = simulatePersonality(userMessage, personality);
    
    return {
      content,
      emotionDetected,
      ...memoryInfo,
    };
  }
}

/**
 * Generate session summary using OpenAI
 */
export async function generateSessionSummary(
  conversationHistory: Message[],
  existingSummary?: string,
  apiConfig?: ApiConfig,
  adminConfig?: AdminConfig
): Promise<string> {
  if (conversationHistory.length === 0) {
    return "æ–°å¯¹è¯";
  }

  // Load API config
  const config = apiConfig || JSON.parse(localStorage.getItem("userApiConfig") || "null");
  
  if (!config) {
    // Fallback: use first user message
    const firstUserMsg = conversationHistory.find(m => m.role === "user");
    if (firstUserMsg) {
      return firstUserMsg.content.substring(0, 50) + (firstUserMsg.content.length > 50 ? "..." : "");
    }
    return "èŠå¤©ä¼šè¯";
  }

  try {
    // Build conversation text
    let conversationText = "";
    for (const msg of conversationHistory.slice(-20)) { // Last 20 messages
      const role = msg.role === "user" ? "ç”¨æˆ·" : "AI";
      conversationText += `${role}: ${msg.content}\n`;
    }

    const prompt = existingSummary
      ? `ä½ æ˜¯ä¸€ä¸ªä¸»é¢˜ç”ŸæˆåŠ©æ‰‹ï¼Œè´Ÿè´£æ ¹æ®æœ€è¿‘çš„å¯¹è¯ç”Ÿæˆä¸€ä¸ªå½“å‰å¯¹è¯çš„ä¸»é¢˜ã€‚\n\næœ€è¿‘çš„å¯¹è¯è®°å½•ï¼š\n"${existingSummary}"\n\n${conversationText}\n\nè¯·æä¾›ä¸€ä¸ªæ›´æ–°åçš„ä¸»é¢˜ï¼ŒåŒ…å«æ–°æ¶ˆæ¯ã€‚ä¸»é¢˜åº”è¯¥ç®€æ´ï¼ˆ1-2å¥è¯ï¼Œæœ€å¤š100ä¸ªå­—ç¬¦ï¼‰ï¼Œæ•æ‰å¯¹è¯çš„ä¸»è¦å†…å®¹ã€‚åªè¿”å›ä¸»é¢˜æ–‡æœ¬ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚`
      : `ä½ æ˜¯ä¸€ä¸ªä¸»é¢˜ç”ŸæˆåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸»é¢˜ï¼ˆ1-2å¥è¯ï¼Œæœ€å¤š100ä¸ªå­—ç¬¦ï¼‰ï¼š\n\n${conversationText}\n\nåªè¿”å›ä¸»é¢˜æ–‡æœ¬ã€‚`;

    const client = createOpenAIClient(config);
    const response = await client.chat.completions.create({
      model: config.model,
      messages: [
        { role: "system", content: "ä½ æ˜¯ä¸€ä¸ªåˆ›å»ºç®€æ´å¯¹è¯ä¸»é¢˜çš„åŠ©æ‰‹ã€‚ä¿æŒä¸»é¢˜åœ¨100ä¸ªå­—ç¬¦ä»¥å†…ã€‚" },
        { role: "user", content: prompt },
      ],
      max_tokens: 50,
      temperature: 0.5,
    });

    let summary = response.choices[0].message.content || "èŠå¤©ä¼šè¯";
    
    // Ensure summary is not too long
    if (summary.length > 100) {
      summary = summary.substring(0, 97) + "...";
    }

    return summary;
  } catch (error) {
    console.error("Failed to generate summary:", error);
    // Fallback
    const firstUserMsg = conversationHistory.find(m => m.role === "user");
    if (firstUserMsg) {
      return firstUserMsg.content.substring(0, 50) + (firstUserMsg.content.length > 50 ? "..." : "");
    }
    return "èŠå¤©ä¼šè¯";
  }
}

/**
 * Decide if personality should be updated based on conversation patterns
 */
export async function decidePersonalityUpdate(
  conversationHistory: Message[],
  currentPersonality: PersonalityConfig,
  messageCount: number,
  sessionSummary: string,
  apiConfig?: ApiConfig,
  adminConfig?: AdminConfig
): Promise<PersonalityUpdateDecision> {
  const MIN_MESSAGES_FOR_UPDATE = 20;

  if (messageCount < MIN_MESSAGES_FOR_UPDATE) {
    return {
      shouldUpdate: false,
      reason: `æ¶ˆæ¯æ•°é‡ä¸è¶³ (éœ€è¦è‡³å°‘ ${MIN_MESSAGES_FOR_UPDATE} æ¡ï¼Œå½“å‰ ${messageCount} æ¡)`,
      confidence: 0.0,
    };
  }

  // Load API config
  const config = apiConfig || JSON.parse(localStorage.getItem("userApiConfig") || "null");
  
  if (!config) {
    // Simple heuristic fallback
    if (messageCount % 50 === 0) {
      return {
        shouldUpdate: true,
        reason: "è¾¾åˆ°50æ¡æ¶ˆæ¯ï¼Œå»ºè®®è€ƒè™‘æ›´æ–°ä¸ªæ€§",
        suggestedPersonality: currentPersonality.systemPrompt,
        confidence: 0.5,
      };
    }
    return {
      shouldUpdate: false,
      reason: "æœªé…ç½® APIï¼Œæ— æ³•è¿›è¡Œé«˜çº§åˆ†æ",
      confidence: 0.0,
    };
  }

  try {
    // Build conversation text
    let conversationText = "";
    for (const msg of conversationHistory.slice(-30)) {
      const role = msg.role === "user" ? "ç”¨æˆ·" : "AI";
      conversationText += `${role}: ${msg.content}\n`;
    }

    const prompt = `ä½ æ­£åœ¨åˆ†æä¸€æ®µå¯¹è¯ï¼Œä»¥ç¡®å®š AI åŠ©æ‰‹çš„ä¸ªæ€§æ˜¯å¦åº”è¯¥æ›´æ–°ã€‚

å½“å‰ä¸ªæ€§æç¤ºè¯: "${currentPersonality.systemPrompt}"
æ¶ˆæ¯æ•°é‡: ${messageCount}
ä¼šè¯æ‘˜è¦: ${sessionSummary}

æœ€è¿‘çš„å¯¹è¯:
${conversationText}

åŸºäºè¿™æ®µå¯¹è¯ï¼Œåˆ†æï¼š
1. å½“å‰ä¸ªæ€§æ˜¯å¦é€‚åˆç”¨æˆ·çš„éœ€æ±‚ï¼Ÿ
2. ç”¨æˆ·æ›´å–œæ¬¢ä»€ä¹ˆæ²Ÿé€šé£æ ¼ï¼Ÿï¼ˆæ­£å¼/éšæ„ï¼Œè¯¦ç»†/ç®€æ´ç­‰ï¼‰
3. å¯¹è¯ä¸­æ˜¯å¦æœ‰ä»»ä½•æ¨¡å¼è¡¨æ˜ä¸åŒçš„ä¸ªæ€§ä¼šæ›´å¥½ï¼Ÿ
4. æ›´æ–°ä¸ªæ€§æ˜¯å¦ä¼šæ”¹å–„ç”¨æˆ·ä½“éªŒï¼Ÿ

è€ƒè™‘ï¼š
- ç”¨æˆ·çš„è¯­è¨€é£æ ¼å’Œæ­£å¼ç¨‹åº¦
- æ­£åœ¨è®¨è®ºçš„è¯é¢˜
- ç”¨æˆ·åå¥½çš„è¯¦ç»†ç¨‹åº¦
- ç”¨æˆ·æ˜¯å¦å¯¹å½“å‰å›å¤æ»¡æ„
- å¯¹è¯è¯é¢˜çš„ä¸€è‡´æ€§

ä»…ä»¥ JSON å¯¹è±¡çš„æ ¼å¼å›å¤ï¼š
{"should_update": true/false, "reason": "è¯´æ˜", "suggested_personality": "æ–°ä¸ªæ€§æç¤ºè¯æˆ– null", "confidence": 0.0-1.0}

suggested_personality åº”è¯¥æ˜¯ä¸€ä¸ªæ¸…æ™°ã€ç®€æ´çš„æç¤ºè¯ï¼Œæè¿° AI åº”è¯¥å¦‚ä½•è¡Œä¸ºã€‚`;

    const client = createOpenAIClient(config);
    const response = await client.chat.completions.create({
      model: config.model,
      messages: [
        { role: "system", content: "ä½ æ˜¯åˆ†æå¯¹è¯å¹¶ç¡®å®šæœ€ä½³ AI ä¸ªæ€§é…ç½®çš„ä¸“å®¶ã€‚å§‹ç»ˆç”¨æœ‰æ•ˆçš„ JSON å›å¤ã€‚" },
        { role: "user", content: prompt },
      ],
      temperature: 0.7,
    });

    const resultText = response.choices[0].message.content || "{}";

    try {
      const result = JSON.parse(resultText);
      
      return {
        shouldUpdate: result.should_update || false,
        reason: result.reason || "æœªçŸ¥åŸå› ",
        suggestedPersonality: result.suggested_personality || undefined,
        confidence: result.confidence || 0.0,
      };
    } catch (parseError) {
      return {
        shouldUpdate: false,
        reason: "æ— æ³•è§£æ AI å“åº”",
        confidence: 0.0,
      };
    }
  } catch (error) {
    console.error("Failed to analyze personality:", error);
    return {
      shouldUpdate: false,
      reason: `åˆ†æé”™è¯¯: ${error instanceof Error ? error.message : "æœªçŸ¥é”™è¯¯"}`,
      confidence: 0.0,
    };
  }
}

/**
 * Make proactive decision based on conversation state
 */
export async function makeProactiveDecision(
  conversationHistory: Message[],
  sessionSummary: string,
  messageCount: number,
  minutesInactive: number,
  apiConfig?: ApiConfig,
  adminConfig?: AdminConfig
): Promise<ProactiveDecision> {
  const INACTIVITY_THRESHOLD = 5;

  if (minutesInactive < INACTIVITY_THRESHOLD) {
    return {
      action: "wait",
      reason: `æ´»åŠ¨æ—¶é—´ä¸è¶³ ${INACTIVITY_THRESHOLD} åˆ†é’Ÿ`,
    };
  }

  // Load API config
  const config = apiConfig || JSON.parse(localStorage.getItem("userApiConfig") || "null");
  
  if (!config) {
    // Simple fallback
    if (messageCount < 5) {
      return {
        action: "wait",
        reason: "å¯¹è¯å¤ªçŸ­ï¼Œæ— æ³•åšå‡ºå†³ç­–",
      };
    }
    return {
      action: "continue",
      reason: "å¯¹è¯å†å²å……è¶³",
      suggestedMessage: "è¿˜æœ‰ä»€ä¹ˆæƒ³èŠçš„å—ï¼Ÿæˆ‘ä¸€ç›´éƒ½åœ¨å“¦ ğŸ˜Š",
    };
  }

  try {
    // Build conversation text
    let conversationText = "";
    for (const msg of conversationHistory.slice(-15)) {
      const role = msg.role === "user" ? "ç”¨æˆ·" : "AI";
      conversationText += `${role}: ${msg.content}\n`;
    }

    const prompt = `ä½ æ­£åœ¨åˆ†æä¸€æ®µå¯¹è¯ï¼Œä»¥å†³å®š AI æ˜¯å¦åº”è¯¥ä¸»åŠ¨ç»§ç»­å¯¹è¯ã€‚

å½“å‰æ‘˜è¦: ${sessionSummary}
æ¶ˆæ¯æ•°é‡: ${messageCount}
ä¸æ´»è·ƒåˆ†é’Ÿæ•°: ${minutesInactive.toFixed(1)}

æœ€è¿‘çš„å¯¹è¯:
${conversationText}

åŸºäºè¿™äº›ä¿¡æ¯ï¼Œå†³å®š AI åº”è¯¥ï¼š
1. 'continue' - ä¸»åŠ¨ç»§ç»­å½“å‰è¯é¢˜ï¼Œç»™å‡ºç›¸å…³çš„åç»­
2. 'new_topic' - å»ºè®®å¼€å§‹ä¸€ä¸ªæ–°çš„ç›¸å…³è¯é¢˜
3. 'wait' - ç­‰å¾…ç”¨æˆ·å›å¤

è€ƒè™‘ï¼š
- å¯¹è¯æ˜¯å¦å¤„äºè‡ªç„¶åœé¡¿ç‚¹ï¼Ÿ
- æ˜¯å¦æœ‰æœªå›ç­”çš„é—®é¢˜æˆ–æœªå®Œæˆçš„æƒ³æ³•ï¼Ÿ
- åç»­æ¶ˆæ¯æ˜¯å¦ä¼šå¢åŠ ä»·å€¼è¿˜æ˜¯æ˜¾å¾—æ‰“æ‰°ï¼Ÿ

ä»…ä»¥ JSON å¯¹è±¡çš„æ ¼å¼å›å¤ï¼š
{"action": "continue|new_topic|wait", "reason": "ç®€çŸ­è¯´æ˜", "suggested_message": "è¦å‘é€çš„æ¶ˆæ¯æˆ– null"}`;

    const client = createOpenAIClient(config);
    const response = await client.chat.completions.create({
      model: config.model,
      messages: [
        { role: "system", content: "ä½ æ˜¯å†³å®š AI å¯¹è¯ç­–ç•¥çš„ä¸“å®¶ã€‚å§‹ç»ˆç”¨æœ‰æ•ˆçš„ JSON å›å¤ã€‚" },
        { role: "user", content: prompt },
      ],
      temperature: 0.7,
    });

    const resultText = response.choices[0].message.content || "{}";

    try {
      const result = JSON.parse(resultText);
      
      return {
        action: result.action || "wait",
        reason: result.reason || "æœªçŸ¥åŸå› ",
        suggestedMessage: result.suggested_message || undefined,
      };
    } catch (parseError) {
      return {
        action: "wait",
        reason: "æ— æ³•è§£æ AI å“åº”",
      };
    }
  } catch (error) {
    console.error("Failed to make proactive decision:", error);
    return {
      action: "wait",
      reason: `å†³ç­–é”™è¯¯: ${error instanceof Error ? error.message : "æœªçŸ¥é”™è¯¯"}`,
    };
  }
}

/**
 * Get default personality
 */
export function getDefaultPersonality(): PersonalityConfig {
  return DEFAULT_PERSONALITY;
}

/**
 * Create a custom personality
 */
export function createPersonality(
  name: string,
  traits: string[],
  systemPrompt: string
): PersonalityConfig {
  return {
    name,
    traits,
    systemPrompt,
  };
}
